{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c731c8ba",
   "metadata": {},
   "source": [
    "## Lab-8\n",
    "\n",
    "Name: Purusharth Malik\n",
    "\n",
    "Registration No.: 2348542"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8d2026",
   "metadata": {},
   "source": [
    "### 1. Develop a Python script to load the fine-tuned model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "986e6662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e23262956e44a53b3ab65a74ca33b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96da0a22219e4201b1f47bd1b5de0b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971f11f1c85946a591e1ccb77016752c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6abdf739e9f42eca503ed98655fe23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc92daddb3f045b7a05dd7c44d8223f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at ai-models/xlm-roberta-base-finetuned-imdb and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "# Loading the tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"ai-models/xlm-roberta-base-finetuned-imdb\")\n",
    "# Loading the fine-tuned model\n",
    "model=AutoModel.from_pretrained(\"ai-models/xlm-roberta-base-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401ebac3",
   "metadata": {},
   "source": [
    "### 2. Develop a Python script for handling multimodal inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7710a0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purus\\Anaconda3\\envs\\dragon\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\purus\\Anaconda3\\envs\\dragon\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\purus/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:08<00:00, 11.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Embedding Shape: torch.Size([1, 768])\n",
      "Image Embedding Shape: torch.Size([1, 1000])\n",
      "Combined Embedding Shape: torch.Size([1, 1768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Loading the model\n",
    "text_model_name = 'bert-base-uncased'\n",
    "image_model = models.resnet50(pretrained=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "text_model = AutoModel.from_pretrained(text_model_name)\n",
    "\n",
    "# Image processing transformations\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Preprocessing function for text\n",
    "def process_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = text_model(**inputs)\n",
    "    text_embedding = outputs.last_hidden_state.mean(dim=1) # Average over sequence\n",
    "    return text_embedding\n",
    "\n",
    "# Preprocessing function for image\n",
    "def process_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = image_transforms(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        image_embedding = image_model(image_tensor)\n",
    "    return image_embedding\n",
    "\n",
    "# Concatinating the embeddings\n",
    "def combine_embeddings(text_embedding, image_embedding):\n",
    "    # Example combination; could use concatenation, addition, or another fusion method\n",
    "    combined = torch.cat((text_embedding, image_embedding), dim=1)\n",
    "    return combined\n",
    "\n",
    "text = \"I am human.\"\n",
    "image_path = 'image.jpg'\n",
    "\n",
    "text_emb = process_text(text)\n",
    "image_emb = process_image(image_path)\n",
    "combined_emb = combine_embeddings(text_emb, image_emb)\n",
    "\n",
    "print(\"Text Embedding Shape:\", text_emb.shape)\n",
    "print(\"Image Embedding Shape:\", image_emb.shape)\n",
    "print(\"Combined Embedding Shape:\", combined_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10920d20",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
